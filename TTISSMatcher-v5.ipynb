{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install and import the required packages:\n",
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} biopython #biopython for alignment and revcom\n",
    "import csv #parses csv\n",
    "import numpy as np \n",
    "from itertools import zip_longest as zip #used to calculate Hamming distance fast\n",
    "from Bio.Seq import Seq #BioTools sequencing\n",
    "from Bio import Align #Used for alignment\n",
    "import pandas as pd #Used for dataframe to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define relevant variables:\n",
    "\n",
    "#Files:\n",
    "FileName =  'GUIDEU2OS_FilteredSRv2.txt' # This is where the hits .txt file is given\n",
    "GuideList = 'Pilot4GUIDEU2OS.csv' # This is where the guides in the form of a .csv are given\n",
    "OutputFileName = 'Processed-GUIDEU2OS_FilteredSRv2.csv' #This is the name you want for the output file\n",
    "\n",
    "#Parameters:\n",
    "NumSamples = 6 #Number of samples\n",
    "StringSearchLength = 25 #Distance on either side of cut site in target string \n",
    "CutDist = 17 #Approximate distance from start of gRNA to cut site\n",
    "MismatchThreshold = 6 #Maximum number of mismatchs at a real site\n",
    "MaxPeakThreshold = 1/3 #Minimum Threshold of reads for a multiple peak within a region compared to largest peak\n",
    "\n",
    "#Delimited Spacer Sequence\n",
    "DelimiterReadout = True #True for GUIDE-seq-like readout, False for just lowercase\n",
    "Delimiter = \"-\" #  Either \"â€’\" or \".\"\n",
    "\n",
    "#TTISS-predicted Indel Distribution:\n",
    "PredictedIndels = False #True if we want to output predicted indels\n",
    "HistosThreshold = 50 #Minimum number of reads on either side in order to calculate distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new numpy array to store values in parsed format\n",
    "\n",
    "#Parse hits file and store entries in OriginalData Numpy array:\n",
    "OriginalData = []\n",
    "with open('GeneralTTISSMatcherHits/'+FileName, 'r') as f:\n",
    "    reader = csv.reader(f, dialect='excel', delimiter='\\t')\n",
    "    for row in reader:\n",
    "        OriginalData.append(row)        \n",
    "OriginalData = np.array(OriginalData)\n",
    "\n",
    "#First, calculate number of samples and initialize new structure\n",
    "(rows, columns) = OriginalData.shape \n",
    "NumSites = int((rows-1)/(NumSamples+1))\n",
    "FilteredData = np.zeros((NumSites+1, NumSamples+3),dtype = object)\n",
    "\n",
    "#Fill out the first row\n",
    "FilteredData[0,0] = OriginalData[0,0]\n",
    "FilteredData[0,1] = \"Sequence\"\n",
    "for r in range(NumSamples):\n",
    "        FilteredData[0,r+2] = OriginalData[r+2,0]\n",
    "FilteredData[0,NumSamples+2]=\"Summed\"\n",
    "\n",
    "#Build the remaining rows \n",
    "for n in range(NumSites):\n",
    "    FilteredData[n+1,0] = OriginalData[(NumSamples+1)*n+1,0] #Added chromosome position to table\n",
    "    Sequence = \"\"\n",
    "    for n1 in range(60):\n",
    "        Sequence += OriginalData[(NumSamples+1)*n+1,n1+1]\n",
    "    FilteredData[n+1,1] = Sequence\n",
    "    SummedArray = np.zeros((1,120),dtype=int)\n",
    "    for n2 in range(NumSamples):\n",
    "        PeakCount = OriginalData[(NumSamples+1)*n+1+n2+1,1:].astype(int)\n",
    "        SummedArray += PeakCount\n",
    "        FilteredData[n+1,2+n2]=PeakCount\n",
    "    FilteredData[n+1,NumSamples+2]=SummedArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create arrays to store guide sequences and match guides to numbers\n",
    "\n",
    "#Parse guides file and store entries in original guides NP array:\n",
    "OriginalGuides = []\n",
    "with open('GeneralTTISSMatcherGuides/'+GuideList, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        OriginalGuides.append(row)        \n",
    "OriginalGuides = np.array(OriginalGuides)\n",
    "\n",
    "#Create list containing all guides and their reverse complements:\n",
    "Guides =  []\n",
    "(ro, col) = OriginalGuides.shape\n",
    "for r in range (ro):\n",
    "    Guides.append(str(OriginalGuides[r, 1]))\n",
    "    seq = Seq(OriginalGuides[r, 1])\n",
    "    Guides.append(str(seq.reverse_complement()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build subroutine to match a given sequences to gRNAs\n",
    "\n",
    "#Define helper function for Hamming distance:\n",
    "def hamdist(str1, str2):    \n",
    "    '''\n",
    "    Inputs: \n",
    "        str1: String of some length\n",
    "        str2: Another string of the same length as str1\n",
    "        \n",
    "    Outputs: \n",
    "        Returns Hamming distance, which is the number of differences\n",
    "        between the two strings\n",
    "    ''' \n",
    "    diffs = 0\n",
    "    for ch1, ch2 in zip(str1, str2):\n",
    "        if ch1 != ch2:\n",
    "            diffs += 1\n",
    "    return diffs\n",
    "\n",
    "#Define helper function that takes in the index of a gRNA string from OriginalGuides and outputs gRNA # and sequence\n",
    "def gRNAfinder(gRNAnumber):\n",
    "    '''\n",
    "    Inputs:\n",
    "        gRNAnumber: index of gRNA sequence from OriginalGuides\n",
    "        \n",
    "    Outputs:\n",
    "        GuideNumber: number of guide from first column of OriginalGuides\n",
    "        RC: Boolean where True means reverse complemented guide and False means correct strand\n",
    "        Sequence: normal strand sequence of original gRNA\n",
    "    '''\n",
    "    RC = False\n",
    "    if gRNAnumber%2 == 1: #If matched guide is the RC of a real guide\n",
    "        GuideNumber = int(OriginalGuides[int((gRNAnumber-1)/2),0])\n",
    "        Sequence = OriginalGuides[int((gRNAnumber-1)/2),1]\n",
    "        RC = True\n",
    "    else:\n",
    "        GuideNumber = int(OriginalGuides[int(gRNAnumber/2),0])\n",
    "        Sequence = OriginalGuides[int(gRNAnumber/2), 1]\n",
    "    '''\n",
    "    #Test cases:\n",
    "    print(gRNAfinder(0))\n",
    "    print(gRNAfinder(1))\n",
    "    print(gRNAfinder(2))\n",
    "    print(gRNAfinder(3))\n",
    "    '''\n",
    "    \n",
    "    return [GuideNumber, RC, Sequence]\n",
    "\n",
    "\n",
    "\n",
    "#Define helper function that takes in a RealString and a gRNA sequence and changes RealString to Delimiter or Lowercase\n",
    "def StringChanger(RealSpacer, guideString):\n",
    "    realseq = RealSpacer\n",
    "    for i in range(20):\n",
    "        if DelimiterReadout:\n",
    "            if realseq[i] == guideString[i]:\n",
    "                newstring = realseq[:i]+Delimiter+realseq[i+1:] \n",
    "                realseq = newstring  \n",
    "        else:\n",
    "            if realseq[i] != guideString[i]:\n",
    "                newstring = realseq[:i]+realseq[i].lower()+realseq[i+1:] #slices to make letter lowercase\n",
    "                realseq = newstring   \n",
    "                \n",
    "    '''\n",
    "    #Test cases:\n",
    "    DelimiterReadout = False\n",
    "    print(StringChanger(\"AAAAAATTTTTTTTTTTTTTTTT\", \"TAAAAAATTTTTTTTTTTTT\"))\n",
    "    DelimiterReadout = True\n",
    "    print(StringChanger(\"AAAAAATTTTTTTTTTTTTTTTT\", \"TAAAAAATTTTTTTTTTTTT\"))\n",
    "    '''\n",
    "    \n",
    "    return realseq\n",
    "\n",
    "\n",
    "\n",
    "#Define function to match string with gRNA:\n",
    "def Matcher(TargetString, CutSite):\n",
    "    '''\n",
    "    Inputs:\n",
    "        TargetString: string of length 2*StringSearchLength\n",
    "        CutSite: Index of predicted cut site, in the middle of target string and a distance CutDist within the gRNA\n",
    "        Implicitly, the gRNA list Guides and the numbers in OriginalGuides are used as well as MismatchThreshold\n",
    "    \n",
    "    Outputs:\n",
    "        SingleMatch: Boolean with True meaning only a single match under mismatch treshold, False if otherwise\n",
    "        MM: Mismatch number between hit(s) with original gRNA(s)\n",
    "        GuideMatch: Number(s) of guide(s) that matched\n",
    "        CrudeSpacer: string(s) within TargetString that matched under the mismatch threshold or best match\n",
    "        RealSpacer: string(s) with sequence in the regular direction of the gRNA with MMs either lowercase\n",
    "                    or matches replaced by the delimiter set in the settings\n",
    "        CutSiteScore: score(s) for how close match(es) is(are) to predicted cut sites\n",
    "\n",
    "        \n",
    "    Possible Errors:\n",
    "        Match PAM is not in the sequence provided\n",
    "    '''\n",
    "    \n",
    "    #Intialize variables for guide matching\n",
    "    BestDist = 25\n",
    "    GuideNumber = 0\n",
    "    Index = 0 \n",
    "    CutSiteScore = 100\n",
    "    \n",
    "    SingleMatch = True\n",
    "    MMs = []\n",
    "    GuideNumberList = []\n",
    "    CrudeSpacerList = []\n",
    "    RealSpacerList = []\n",
    "    CutSiteScoreList = []\n",
    "\n",
    "    #Iterate over all guides to find best match(es)\n",
    "    for g in range(len(Guides)):        \n",
    "        #Iterate over all possible 20 base pair windows\n",
    "        SpacerBestDist = 25\n",
    "        SpacerBestMatch = 0\n",
    "        for m in range(len(TargetString)-20):            \n",
    "            CurrentString = TargetString[m:m+20] #Defines current test window\n",
    "            SpacerTestDist = hamdist(CurrentString, Guides[g]) # Calculates Hamdist between test window and current guide\n",
    "            if SpacerTestDist <= SpacerBestDist:\n",
    "                SpacerBestDist = SpacerTestDist\n",
    "                SpacerBestMatch = m\n",
    "        t=SpacerBestMatch\n",
    "        TestDist = SpacerBestDist\n",
    "        #Precomputations for gRNA crude and real spacer\n",
    "        if TestDist<= MismatchThreshold or TestDist <= BestDist:\n",
    "            GuideNumber = gRNAfinder(g)[0]\n",
    "            if gRNAfinder(g)[1] == False: #Not RC\n",
    "                if TargetString[t:t+23] != \"\":\n",
    "                    AlignedString = TargetString[t:t+23]\n",
    "                    RealString = AlignedString\n",
    "                else: #covers the corner case where PAM is not in the string\n",
    "                    AlignedString = TargetString[t:t+20]+\" PAM INDEX ERROR \"+str(t)\n",
    "                    RealString = TargetString[t:t+20]\n",
    "                CutSiteScore = t + CutDist - CutSite  \n",
    "            else:\n",
    "                if TargetString[t-3:t+20] != \"\":\n",
    "                    AlignedString = TargetString[t-3:t+20]\n",
    "                    SpacerSeq = Seq(TargetString[t-3:t+20])\n",
    "                    RealString = str(SpacerSeq.reverse_complement())\n",
    "                else: #covers the corner case where PAM is not in the string\n",
    "                    AlignedString = TargetString[t:t+20]+\" PAM INDEX ERROR RC \"+str(t)\n",
    "                    SpacerSeq = Seq(TargetString[t:t+20])\n",
    "                    RealString = str(SpacerSeq.reverse_complement())\n",
    "                CutSiteScore = t + 20 - CutDist - CutSite  \n",
    "            RealString = StringChanger(RealString, gRNAfinder(g)[2]) #Convert real string to good format\n",
    "\n",
    "        '''\n",
    "        5 cases here: \n",
    "            1) TestDist is > MismatchThreshold and > BestDist so no change\n",
    "            2) TestDist is > MismatchThreshold and < BestDist so BestDist and all variables updated/replaced\n",
    "            3) TestDist is > MismatchThreshold and = BestDist so BestDist and all variables appended\n",
    "            4) TestDist is <= MismatchThreshold and BestDist > MismatchThreshold so all variables updated/replaced\n",
    "            5) TestDist is <= MismatchThreshold and BestDist <= MismatchThreshold so all variables appended\n",
    "        We are done with Case 1 (no change), and will deal with cases 3 and 5 first\n",
    "        '''\n",
    "\n",
    "        #Case 3 and 5\n",
    "        if (TestDist <= MismatchThreshold and BestDist <= MismatchThreshold) or (TestDist == BestDist):\n",
    "            SingleMatch = False #Update SingleMatch to reflect multiple matches\n",
    "            MMs.append(TestDist)\n",
    "            GuideNumberList.append(GuideNumber)\n",
    "            CrudeSpacerList.append(AlignedString)\n",
    "            RealSpacerList.append(RealString)\n",
    "            CutSiteScoreList.append(CutSiteScore)\n",
    "\n",
    "        #Case 2 and 4 (BestDist is now implictly > MismatchThreshold if first condition satisfied)\n",
    "        elif (TestDist <= MismatchThreshold) or (TestDist < BestDist):\n",
    "            BestDist = TestDist #Update best distance, it is now <= MismatchThreshold\n",
    "            SingleMatch = True #Update SingleMatch to reflect single match \n",
    "            MMs = [TestDist]\n",
    "            GuideNumberList = [GuideNumber]\n",
    "            CrudeSpacerList = [AlignedString]\n",
    "            RealSpacerList = [RealString]\n",
    "            CutSiteScoreList = [CutSiteScore]      \n",
    "            \n",
    "    '''\n",
    "    #Test Cases\n",
    "    match1 = Matcher('CTCTTCCCCAGCCCAGCCTCTTCCTGGAGTTCCTTCCTGGGACCTGCAGTGCCAAGTGCT', 30)\n",
    "    print(match1)\n",
    "    print(match1[0])\n",
    "    for i in range(5):\n",
    "        print(match1[i+1][0])\n",
    "    print(Matcher('AAATATTCCTTCTGTTCCTTTCTTTCTTTCTTCTCCTGCTGGGACTCCTATTTTATATAT', 30))\n",
    "    print(Matcher('GAGGCTCAGGCAAGCCGCGCTGAGCCTTTCTTCTCCTTCCCGGACTCCTCACCACCCATC', 30))\n",
    "    '''\n",
    "    \n",
    "    return [SingleMatch, MMs, GuideNumberList, CrudeSpacerList, RealSpacerList, CutSiteScoreList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build subroutine to find (a) peak(s) given aggregate AS and S peak histos\n",
    "def PeakFinder(peakhistosum):\n",
    "    '''\n",
    "    Inputs:\n",
    "        peakhistosum: aggregated AS and S reads from all tracks\n",
    "            likely of form FilteredData[i,2+NumSamples][0]\n",
    "    Outputs:\n",
    "        FinalIndices: List of predicted cut site positions \n",
    "        \n",
    "    Assumptions: \n",
    "        If there are multiple peaks, then other peaks will have convo sum at least MaxPeakThreshold of top peak\n",
    "        and at least 3 reads. We assume that convolution filter will be enough to pick up peaks\n",
    "        Things within a 6 bp window of either side of a main peak are also the same cut\n",
    "    '''\n",
    "    #First break up peakhistosum into sense and antisense arrays\n",
    "    peakhistosum = np.array(peakhistosum)\n",
    "    Shistos = peakhistosum[0:60]\n",
    "    AShistos = peakhistosum[60:]\n",
    "    \n",
    "    #Initialize arrays\n",
    "    Sindex = []\n",
    "    ASindex = []\n",
    "    Indices = []\n",
    "    FinalIndices = []\n",
    "    \n",
    "    #Define filters for convolution and attempted matched filtering\n",
    "    Sfilter = [10, 50, 100]\n",
    "    ASfilter = [100, 50, 10]\n",
    "    \n",
    "    #Convolve histos with filters\n",
    "    Sconvo = np.convolve(Shistos, Sfilter, 'valid')\n",
    "    ASconvo = np.convolve(AShistos, ASfilter, 'valid')\n",
    "    \n",
    "    #If there are reads on a given side, find first max peak\n",
    "    if np.count_nonzero(Sconvo)>0:\n",
    "        maxSpeak = np.amax(Sconvo)\n",
    "        maxSindex = np.where(Sconvo == maxSpeak)\n",
    "        for m in range(len(maxSindex)):\n",
    "            Sindex.append(maxSindex[m][0]) \n",
    "            #Replace 5 on either side of peak with 0\n",
    "            for r in range(21):\n",
    "                try:\n",
    "                    Sconvo[maxSindex[m]-10+r]=0\n",
    "                except Exception:\n",
    "                    pass\n",
    "        #Reset all convolution sums < MaxPeakThreshold of max to 0\n",
    "        Sconvo[Sconvo<maxSpeak*MaxPeakThreshold]=0\n",
    "        Sconvo[Sconvo<500]=0\n",
    "    else:\n",
    "        Sindex.append(\"None\")\n",
    "        \n",
    "    if np.count_nonzero(ASconvo)>0:\n",
    "        maxASpeak = np.amax(ASconvo)\n",
    "        maxASindex = np.where(ASconvo == maxASpeak)\n",
    "        for n in range(len(maxASindex)):\n",
    "            ASindex.append(maxASindex[n][0]+2) \n",
    "            #Replace 5 on either side of peak with 0\n",
    "            for r in range(21):\n",
    "                try:\n",
    "                    ASconvo[maxASindex[n]-10+r]=0\n",
    "                except Exception:\n",
    "                    pass\n",
    "        #Reset all convolution sums < MaxPeakThreshold of max to 0\n",
    "        ASconvo[ASconvo<maxASpeak*MaxPeakThreshold]=0\n",
    "        ASconvo[ASconvo<500]=0\n",
    "    else:\n",
    "        ASindex.append(\"None\")\n",
    "    \n",
    "    #If there are any peaks remaining, store them:\n",
    "    if np.count_nonzero(Sconvo)>0:\n",
    "        maxSindex = np.where(Sconvo > 0)\n",
    "        for m in range(len(maxSindex)):\n",
    "            if Sconvo[maxSindex[m][0]]!=0:\n",
    "                Sindex.append(maxSindex[m][0]) \n",
    "                #Replace 5 on either side of peak with 0\n",
    "                for r in range(21):\n",
    "                    try:\n",
    "                        Sconvo[maxSindex[m]-10+r]=0\n",
    "                    except Exception:\n",
    "                        pass\n",
    "    if np.count_nonzero(ASconvo)>0:\n",
    "        maxASindex = np.where(ASconvo > 0)\n",
    "        for n in range(len(maxASindex)):\n",
    "            if ASconvo[maxASindex[n][0]]!=0:\n",
    "                ASindex.append(maxASindex[n][0]+2) \n",
    "                #Replace 5 on either side of peak with 0\n",
    "                for r in range(21):\n",
    "                    try:\n",
    "                        ASconvo[maxASindex[n]-10+r]=0\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                \n",
    "    #We now have two lists of indices, one generated by sense reads and the other by antisense reads. \n",
    "    #We now need to merge them. Any indices within 5 of each other are grouped together.\n",
    "    if Sindex != [\"None\"] and ASindex !=[\"None\"]:\n",
    "        Indices = Sindex+ASindex\n",
    "        Indices = sorted(Indices)\n",
    "    elif ASindex !=[\"None\"]:\n",
    "        Indices = sorted(ASindex)\n",
    "    elif Sindex !=[\"None\"]:\n",
    "        Indices = sorted(Sindex)\n",
    "    #Bad coding practice, but we assume that we were not given an array of 0s... so no edge case here\n",
    "    \n",
    "    FinalIndices.append(Indices[0])\n",
    "    lastadded = Indices[0]\n",
    "    for i in range(len(Indices)-1): \n",
    "        if Indices[i+1]-lastadded>10:\n",
    "            FinalIndices.append(Indices[i+1])\n",
    "            lastadded = Indices[i+1]\n",
    "            \n",
    "    '''\n",
    "    #Test Cases\n",
    "\n",
    "    for i in range(50):\n",
    "        print(np.array(FilteredData[i+1,2+NumSamples][0]))\n",
    "        print(PeakFinder(FilteredData[i+1,2+NumSamples][0]))\n",
    "    '''\n",
    "    return FinalIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed 50 out of 3196 putative cut sites\n",
      "Analyzed 100 out of 3196 putative cut sites\n",
      "Analyzed 150 out of 3196 putative cut sites\n",
      "Analyzed 200 out of 3196 putative cut sites\n",
      "Analyzed 250 out of 3196 putative cut sites\n",
      "Analyzed 300 out of 3196 putative cut sites\n",
      "Analyzed 350 out of 3196 putative cut sites\n",
      "Analyzed 400 out of 3196 putative cut sites\n",
      "Analyzed 450 out of 3196 putative cut sites\n",
      "Analyzed 500 out of 3196 putative cut sites\n",
      "Analyzed 550 out of 3196 putative cut sites\n",
      "Analyzed 600 out of 3196 putative cut sites\n",
      "Analyzed 650 out of 3196 putative cut sites\n",
      "Analyzed 700 out of 3196 putative cut sites\n",
      "Analyzed 750 out of 3196 putative cut sites\n",
      "Analyzed 800 out of 3196 putative cut sites\n",
      "Analyzed 850 out of 3196 putative cut sites\n",
      "Analyzed 900 out of 3196 putative cut sites\n",
      "Analyzed 950 out of 3196 putative cut sites\n",
      "Analyzed 1000 out of 3196 putative cut sites\n",
      "Analyzed 1050 out of 3196 putative cut sites\n",
      "Analyzed 1100 out of 3196 putative cut sites\n",
      "Analyzed 1150 out of 3196 putative cut sites\n",
      "Analyzed 1200 out of 3196 putative cut sites\n",
      "Analyzed 1250 out of 3196 putative cut sites\n",
      "Analyzed 1300 out of 3196 putative cut sites\n",
      "Analyzed 1350 out of 3196 putative cut sites\n",
      "Analyzed 1400 out of 3196 putative cut sites\n",
      "Analyzed 1450 out of 3196 putative cut sites\n",
      "Analyzed 1500 out of 3196 putative cut sites\n",
      "Analyzed 1550 out of 3196 putative cut sites\n",
      "Analyzed 1600 out of 3196 putative cut sites\n",
      "Analyzed 1650 out of 3196 putative cut sites\n",
      "Analyzed 1700 out of 3196 putative cut sites\n",
      "Analyzed 1750 out of 3196 putative cut sites\n",
      "Analyzed 1800 out of 3196 putative cut sites\n",
      "Analyzed 1850 out of 3196 putative cut sites\n",
      "Analyzed 1900 out of 3196 putative cut sites\n",
      "Analyzed 1950 out of 3196 putative cut sites\n",
      "Analyzed 2000 out of 3196 putative cut sites\n",
      "Analyzed 2050 out of 3196 putative cut sites\n",
      "Analyzed 2100 out of 3196 putative cut sites\n",
      "Analyzed 2150 out of 3196 putative cut sites\n",
      "Analyzed 2200 out of 3196 putative cut sites\n",
      "Analyzed 2250 out of 3196 putative cut sites\n",
      "Analyzed 2300 out of 3196 putative cut sites\n",
      "Analyzed 2350 out of 3196 putative cut sites\n",
      "Analyzed 2400 out of 3196 putative cut sites\n",
      "Analyzed 2450 out of 3196 putative cut sites\n",
      "Analyzed 2500 out of 3196 putative cut sites\n",
      "Analyzed 2550 out of 3196 putative cut sites\n",
      "Analyzed 2600 out of 3196 putative cut sites\n",
      "Analyzed 2650 out of 3196 putative cut sites\n",
      "Analyzed 2700 out of 3196 putative cut sites\n",
      "Analyzed 2750 out of 3196 putative cut sites\n",
      "Analyzed 2800 out of 3196 putative cut sites\n",
      "Analyzed 2850 out of 3196 putative cut sites\n",
      "Analyzed 2900 out of 3196 putative cut sites\n",
      "Analyzed 2950 out of 3196 putative cut sites\n",
      "Analyzed 3000 out of 3196 putative cut sites\n",
      "Analyzed 3050 out of 3196 putative cut sites\n",
      "Analyzed 3100 out of 3196 putative cut sites\n",
      "Analyzed 3150 out of 3196 putative cut sites\n"
     ]
    }
   ],
   "source": [
    "#We can now go row-by-row (site-by-site) through FilteredData and calculate all we need to, storing into FinalData \n",
    "\n",
    "#First, let's calculate the total number of predicted cut sites and their indices\n",
    "IndiceList = []\n",
    "PutativeSiteCounter = 0\n",
    "for a in range(NumSites):\n",
    "    NewIndices = PeakFinder(FilteredData[a+1,2+NumSamples][0])\n",
    "    PutativeSiteCounter += len(NewIndices)\n",
    "    IndiceList.append(NewIndices)\n",
    "\n",
    "#Intialize and fill in first row of our final structure:\n",
    "if PredictedIndels:\n",
    "    pass #TODO LATER\n",
    "    #Need to integrate from previous code\n",
    "else:\n",
    "    FinalData = np.ndarray(shape=(PutativeSiteCounter+1, NumSamples+9), dtype=object)\n",
    "    FinalData[0,0] = \"Genome Position\"\n",
    "    for i in range(NumSamples+2):\n",
    "        FinalData[0,i+1]=FilteredData[0,i+1]\n",
    "    FinalData[0, NumSamples+3] = \"Single Match?\"\n",
    "    FinalData[0, NumSamples+4] = \"MMs\"\n",
    "    FinalData[0, NumSamples+5] = \"gRNA\"\n",
    "    FinalData[0, NumSamples+6] = \"Crude Spacer\"\n",
    "    FinalData[0, NumSamples+7] = \"Real Spacer\"\n",
    "    FinalData[0, NumSamples+8] = \"Cut Site Score\"\n",
    "\n",
    "#Now, we iterate through all predicted cut sites and fill in that row:\n",
    "AnalyzedCutSiteCounter = 0 \n",
    "\n",
    "for a in range(NumSites): #Iterating through all original cut sites\n",
    "    for i in range(len(IndiceList[a])): #Iterating through all peaks at each cut site\n",
    "        FinalData[AnalyzedCutSiteCounter+1,0]=FilteredData[a+1,0] #Fill in position\n",
    "        FinalData[AnalyzedCutSiteCounter+1,1]=FilteredData[a+1,1] #Fill in local seq\n",
    "        for j in range(NumSamples):\n",
    "            FinalData[AnalyzedCutSiteCounter+1,2+j]=sum(FilteredData[a+1,2+j]) #Fill in summed reads per track\n",
    "        FinalData[AnalyzedCutSiteCounter+1,2+NumSamples]=sum(FilteredData[a+1,2+NumSamples][0]) #Fill in summed\n",
    "        #Try to pull out local seq for matching\n",
    "        CurrentIndex = IndiceList[a][i]\n",
    "        if CurrentIndex <=StringSearchLength:\n",
    "            LocalSpacerSeq = FilteredData[a+1,1][0:CurrentIndex+StringSearchLength+1]\n",
    "            MatchedOutput = Matcher(LocalSpacerSeq, CurrentIndex)\n",
    "        elif CurrentIndex >=60-StringSearchLength:\n",
    "            LocalSpacerSeq = FilteredData[a+1,1][CurrentIndex-StringSearchLength:]\n",
    "            MatchedOutput = Matcher(LocalSpacerSeq, StringSearchLength+1)\n",
    "        else: \n",
    "            LocalSpacerSeq =  FilteredData[a+1,1][IndiceList[a][i]-StringSearchLength:IndiceList[a][i]+StringSearchLength+1]\n",
    "            MatchedOutput = Matcher(LocalSpacerSeq, StringSearchLength+1)\n",
    "        FinalData[AnalyzedCutSiteCounter+1, NumSamples+3+0]=MatchedOutput[0]\n",
    "        if MatchedOutput[0] == True:\n",
    "            if MatchedOutput[1] != []:\n",
    "                for k in range(5):\n",
    "                    FinalData[AnalyzedCutSiteCounter+1, NumSamples+4+k] = MatchedOutput[k+1][0]\n",
    "            else:\n",
    "                for k in range(5):\n",
    "                    FinalData[AnalyzedCutSiteCounter+1, NumSamples+4+k] = \"ERROR\" \n",
    "        else:\n",
    "            for l in range(5):\n",
    "                FinalData[AnalyzedCutSiteCounter+1, NumSamples+4+l] = MatchedOutput[l+1]\n",
    "        AnalyzedCutSiteCounter += 1\n",
    "        if AnalyzedCutSiteCounter % 50 == 0:\n",
    "            print(\"Analyzed \"+str(AnalyzedCutSiteCounter)+\" out of \" + str(PutativeSiteCounter)+\" putative cut sites\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writes final matrix to csv file of choice:\n",
    "df = pd.DataFrame(FinalData)\n",
    "df.to_csv('GeneralTTISSMatcherProcessed/'+OutputFileName, index = False, header = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
